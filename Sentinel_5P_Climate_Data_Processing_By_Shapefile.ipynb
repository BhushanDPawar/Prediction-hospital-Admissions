{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd487b3e",
   "metadata": {},
   "source": [
    "# Sentinel-5P Dataset: Downloading and Processing for Malta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb574d02",
   "metadata": {},
   "source": [
    "Description are given here: https://meeo-s5p.s3.amazonaws.com/index.html/kD2NDFdjCNyAQ2W89uMHSKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb32444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have wget installed, just uncomment the following line and run it\n",
    "#! pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3b0e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\Bhushan_SP5_ERA5_Processing_Script\n"
     ]
    }
   ],
   "source": [
    "# Getting the current working dirctory\n",
    "import os, sys # will be used to create and modify file name and save it\n",
    "pwd = os.getcwd()\n",
    "print(pwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83a8da",
   "metadata": {},
   "source": [
    "# Downloading Sulphur Dioxide (SO2)\n",
    "https://meeo-s5p.s3.amazonaws.com/index.html/kD2NDFdjCNyAQ2W89uMHSKP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dfdec2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# First changing the working directory to current as all the paths are set relative to current working directory\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"This script automatically generates the url and downlaod the CHIRPS Rainfall dataset\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwget\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wget'"
     ]
    }
   ],
   "source": [
    "# First changing the working directory to current as all the paths are set relative to current working directory\n",
    "\"\"\"This script automatically generates the url and downlaod the CHIRPS Rainfall dataset\"\"\"\n",
    "import wget\n",
    "import requests\n",
    "import json\n",
    "import os, sys\n",
    "from calendar import monthrange\n",
    "#os.chdir(pwd)\n",
    "#Specify the start and end year of dataset\n",
    "start_year =  2021 # yyyy\n",
    "end_year = 2021\n",
    "# Specify the working directory\n",
    "path = \"./\"\n",
    "download_dir = os.path.join(path,\"S5P_SO2\")\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "# sample link: https://meeo-s5p.s3.amazonaws.com/OFFL/L2__AER_AI/2021/04/01/S5P_OFFL_L2__AER_AI_20210401T015128_20210401T033259_17957_01_010400_20210402T153421.nc\n",
    "base_site = \"https://meeo-s5p.s3.amazonaws.com/OFFL/\"\n",
    "#json_file = \"https://meeo-s5p.s3.amazonaws.com/OFFL/L2__SO2___/2021/04/01/catalog.json\"\n",
    "################################################\n",
    "for year in range(start_year, end_year+1,1):\n",
    "    for month in range(1,13):\n",
    "        month = str(month).zfill(2)\n",
    "        #print(month)\n",
    "        for day in range(1,(monthrange(year, int(month))[1])+1):\n",
    "            day = \"{:02d}\".format(day)\n",
    "            #print(day)\n",
    "            json_url = \"{}{}/{}/{}/{}/{}\".format(base_site,\"L2__SO2___\",year,month,day,\"catalog.json\")\n",
    "            file_ext  = \".nc\"\n",
    "            #print(json_file)\n",
    "            #url = \"https://meeo-s5p.s3.amazonaws.com/OFFL/L2__AER_AI/2021/04/01/catalog.json\"\n",
    "            response = requests.get(json_url)\n",
    "            if response.status_code == 200:\n",
    "                data = json.loads(response.text)\n",
    "                #print(dir(data))\n",
    "                #print(list(data[\"links\"]))\n",
    "                urls = data[\"links\"]\n",
    "                for url in range(3,len(urls)):\n",
    "                    #print(urls[url]['href'])\n",
    "                    json_url = urls[url]['href']\n",
    "                    download_url, ext = os.path.splitext(json_url)\n",
    "                    file_name = download_url.split('/')[-1]\n",
    "                    file_name = \"{}{}\".format(file_name,file_ext)\n",
    "                    download_url = \"{}{}\".format(download_url,file_ext)\n",
    "                    #print(download_url)\n",
    "                    print(\"{}{}\".format(\"\\nDownloading.....\",file_name))\n",
    "                    wget.download(download_url,download_dir)\n",
    "                    print(\"{}{}\".format(file_name,\" Downloaded Successfully!\"))\n",
    "                \n",
    "            else:\n",
    "                print(\"No Data Available for the date {}-{}-{}\".format(year,month,day))\n",
    "print(\"\\nAll Data Have Been Downloaded Successfully!\\n\\n\\n\")           \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f85fb77",
   "metadata": {},
   "source": [
    "# Downloading Carbon Monoxide (CO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6085831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First changing the working directory to current as all the paths are set relative to current working directory\n",
    "\"\"\"This script automatically generates the url and downlaod the CHIRPS Rainfall dataset\"\"\"\n",
    "import wget\n",
    "import requests\n",
    "import json\n",
    "import os, sys\n",
    "from calendar import monthrange\n",
    "#os.chdir(pwd)\n",
    "#Specify the start and end year of dataset\n",
    "start_year =  2021 # yyyy\n",
    "end_year = 2021\n",
    "# Specify the working directory\n",
    "path = \"./\"\n",
    "download_dir = os.path.join(path,\"S5P_CO\")\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "# sample link: https://meeo-s5p.s3.amazonaws.com/OFFL/L2__AER_AI/2021/04/01/S5P_OFFL_L2__AER_AI_20210401T015128_20210401T033259_17957_01_010400_20210402T153421.nc\n",
    "base_site = \"https://meeo-s5p.s3.amazonaws.com/OFFL/\"\n",
    "#json_file = \"https://meeo-s5p.s3.amazonaws.com/OFFL/L2__CO____/2021/04/01/catalog.json\"\n",
    "################################################\n",
    "for year in range(start_year, end_year+1,1):\n",
    "    for month in range(1,13):\n",
    "        month = str(month).zfill(2)\n",
    "        #print(month)\n",
    "        for day in range(1,(monthrange(year, int(month))[1])+1):\n",
    "            day = \"{:02d}\".format(day)\n",
    "            #print(day)\n",
    "            json_url = \"{}{}/{}/{}/{}/{}\".format(base_site,\"L2__CO____\",year,month,day,\"catalog.json\")\n",
    "            file_ext  = \".nc\"\n",
    "            #print(json_file)\n",
    "            #url = \"https://meeo-s5p.s3.amazonaws.com/OFFL/L2__AER_AI/2021/04/01/catalog.json\"\n",
    "            response = requests.get(json_url)\n",
    "            if response.status_code == 200:\n",
    "                data = json.loads(response.text)\n",
    "                #print(dir(data))\n",
    "                #print(list(data[\"links\"]))\n",
    "                urls = data[\"links\"]\n",
    "                for url in range(3,len(urls)):\n",
    "                    #print(urls[url]['href'])\n",
    "                    json_url = urls[url]['href']\n",
    "                    download_url, ext = os.path.splitext(json_url)\n",
    "                    file_name = download_url.split('/')[-1]\n",
    "                    file_name = \"{}{}\".format(file_name,file_ext)\n",
    "                    download_url = \"{}{}\".format(download_url,file_ext)\n",
    "                    #print(download_url)\n",
    "                    print(\"{}{}\".format(\"\\nDownloading.....\",file_name))\n",
    "                    wget.download(download_url,download_dir)\n",
    "                    print(\"{}{}\".format(file_name,\" Downloaded Successfully!\"))\n",
    "                \n",
    "            else:\n",
    "                print(\"No Data Available for the date {}-{}-{}\".format(year,month,day))\n",
    "print(\"\\nAll Data Have Been Downloaded Successfully!\\n\\n\\n\")           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf5899",
   "metadata": {},
   "source": [
    "# Downloading Cloud fraction, top pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b85673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First changing the working directory to current as all the paths are set relative to current working directory\n",
    "\"\"\"This script automatically generates the url and downlaod the CHIRPS Rainfall dataset\"\"\"\n",
    "import wget\n",
    "import requests\n",
    "import json\n",
    "import os, sys\n",
    "from calendar import monthrange\n",
    "#os.chdir(pwd)\n",
    "#Specify the start and end year of dataset\n",
    "start_year =  2021 # yyyy\n",
    "end_year = 2021\n",
    "# Specify the working directory\n",
    "path = \"./\"\n",
    "download_dir = os.path.join(path,\"S5P_CLOUD\")\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "# sample link: https://meeo-s5p.s3.amazonaws.com/OFFL/L2__AER_AI/2021/04/01/S5P_OFFL_L2__AER_AI_20210401T015128_20210401T033259_17957_01_010400_20210402T153421.nc\n",
    "base_site = \"https://meeo-s5p.s3.amazonaws.com/OFFL/\"\n",
    "#json_file = \"https://meeo-s5p.s3.amazonaws.com/OFFL/L2__CLOUD_/2021/04/01/catalog.json\"\n",
    "################################################\n",
    "for year in range(start_year, end_year+1,1):\n",
    "    for month in range(1,13):\n",
    "        month = str(month).zfill(2)\n",
    "        #print(month)\n",
    "        for day in range(1,(monthrange(year, int(month))[1])+1):\n",
    "            day = \"{:02d}\".format(day)\n",
    "            #print(day)\n",
    "            json_url = \"{}{}/{}/{}/{}/{}\".format(base_site,\"L2__CLOUD_\",year,month,day,\"catalog.json\")\n",
    "            file_ext  = \".nc\"\n",
    "            #print(json_file)\n",
    "            #url = \"https://meeo-s5p.s3.amazonaws.com/OFFL/L2__AER_AI/2021/04/01/catalog.json\"\n",
    "            response = requests.get(json_url)\n",
    "            if response.status_code == 200:\n",
    "                data = json.loads(response.text)\n",
    "                #print(dir(data))\n",
    "                #print(list(data[\"links\"]))\n",
    "                urls = data[\"links\"]\n",
    "                for url in range(3,len(urls)):\n",
    "                    #print(urls[url]['href'])\n",
    "                    json_url = urls[url]['href']\n",
    "                    download_url, ext = os.path.splitext(json_url)\n",
    "                    file_name = download_url.split('/')[-1]\n",
    "                    file_name = \"{}{}\".format(file_name,file_ext)\n",
    "                    download_url = \"{}{}\".format(download_url,file_ext)\n",
    "                    #print(download_url)\n",
    "                    print(\"{}{}\".format(\"\\nDownloading.....\",file_name))\n",
    "                    wget.download(download_url,download_dir)\n",
    "                    print(\"{}{}\".format(file_name,\" Downloaded Successfully!\"))\n",
    "                \n",
    "            else:\n",
    "                print(\"No Data Available for the date {}-{}-{}\".format(year,month,day))\n",
    "print(\"\\nAll Data Have Been Downloaded Successfully!\\n\\n\\n\")           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f48cad",
   "metadata": {},
   "source": [
    "# Downloading UV Aerosol Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04274fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First changing the working directory to current as all the paths are set relative to current working directory\n",
    "\"\"\"This script automatically generates the url and downlaod the CHIRPS Rainfall dataset\"\"\"\n",
    "import wget\n",
    "import requests\n",
    "import json\n",
    "import os, sys\n",
    "from calendar import monthrange\n",
    "#os.chdir(pwd)\n",
    "#Specify the start and end year of dataset\n",
    "start_year =  2021 # yyyy\n",
    "end_year = 2021\n",
    "# Specify the working directory\n",
    "path = \"./\"\n",
    "download_dir = os.path.join(path,\"S5P_AER_AI\")\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "#Varialbes = ['L2__AER_AI',]\n",
    "# sample link: https://meeo-s5p.s3.amazonaws.com/OFFL/L2__AER_AI/2021/04/01/S5P_OFFL_L2__AER_AI_20210401T015128_20210401T033259_17957_01_010400_20210402T153421.nc\n",
    "base_site = \"https://meeo-s5p.s3.amazonaws.com/OFFL/\"\n",
    "json_file = \"https://meeo-s5p.s3.amazonaws.com/OFFL/L2__AER_AI/2021/04/01/catalog.json\"\n",
    "# Sample file name: S5P_OFFL_L2__AER_AI_20210401T015128_20210401T033259_17957_01_010400_20210402T153421.nc\n",
    "################################################\n",
    "for year in range(start_year, end_year+1,1):\n",
    "    for month in range(1,13):\n",
    "        month = str(month).zfill(2)\n",
    "        #print(month)\n",
    "        for day in range(1,(monthrange(year, int(month))[1])+1):\n",
    "            day = \"{:02d}\".format(day)\n",
    "            #print(day)\n",
    "            json_url = \"{}{}/{}/{}/{}/{}\".format(base_site,\"L2__AER_AI\",year,month,day,\"catalog.json\")\n",
    "            file_ext  = \".nc\"\n",
    "            #print(json_file)\n",
    "            #url = \"https://meeo-s5p.s3.amazonaws.com/OFFL/L2__AER_AI/2021/04/01/catalog.json\"\n",
    "            response = requests.get(json_url)\n",
    "            if response.status_code == 200:\n",
    "                data = json.loads(response.text)\n",
    "                #print(dir(data))\n",
    "                #print(list(data[\"links\"]))\n",
    "                urls = data[\"links\"]\n",
    "                for url in range(3,len(urls)):\n",
    "                    #print(urls[url]['href'])\n",
    "                    json_url = urls[url]['href']\n",
    "                    download_url, ext = os.path.splitext(json_url)\n",
    "                    file_name = download_url.split('/')[-1]\n",
    "                    file_name = \"{}{}\".format(file_name,file_ext)\n",
    "                    download_url = \"{}{}\".format(download_url,file_ext)\n",
    "                    #print(download_url)\n",
    "                    print(\"{}{}\".format(\"\\nDownloading.....\",file_name))\n",
    "                    wget.download(download_url,download_dir)\n",
    "                    print(\"{}{}\".format(file_name,\" Downloaded Successfully!\"))\n",
    "                \n",
    "            else:\n",
    "                print(\"No Data Available for the date {}-{}-{}\".format(year,month,day))\n",
    "print(\"\\nAll Data Have Been Downloaded Successfully!\\n\\n\\n\")           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf154579",
   "metadata": {},
   "source": [
    "# Downloading Climate Data From ECMWF (ERA5)\n",
    "https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "\n",
    "c = cdsapi.Client()\n",
    "\n",
    "c.retrieve(\n",
    "    'reanalysis-era5-single-levels',\n",
    "    {\n",
    "        'product_type': 'reanalysis',\n",
    "        'variable': [\n",
    "            '10m_u_component_of_wind', '10m_v_component_of_wind', '2m_temperature',\n",
    "            'surface_pressure',\n",
    "        ],\n",
    "        'year': [\n",
    "            '2019', '2020', '2021',\n",
    "        ],\n",
    "        'month': [\n",
    "            '01', '02', '03',\n",
    "            '04', '05', '06',\n",
    "            '07', '08', '09',\n",
    "            '10', '11', '12',\n",
    "        ],\n",
    "        'day': [\n",
    "            '01', '02', '03',\n",
    "            '04', '05', '06',\n",
    "            '07', '08', '09',\n",
    "            '10', '11', '12',\n",
    "            '13', '14', '15',\n",
    "            '16', '17', '18',\n",
    "            '19', '20', '21',\n",
    "            '22', '23', '24',\n",
    "            '25', '26', '27',\n",
    "            '28', '29', '30',\n",
    "            '31',\n",
    "        ],\n",
    "        'time': [\n",
    "            '00:00', '01:00', '02:00',\n",
    "            '03:00', '04:00', '05:00',\n",
    "            '06:00', '07:00', '08:00',\n",
    "            '09:00', '10:00', '11:00',\n",
    "            '12:00', '13:00', '14:00',\n",
    "            '15:00', '16:00', '17:00',\n",
    "            '18:00', '19:00', '20:00',\n",
    "            '21:00', '22:00', '23:00',\n",
    "        ],\n",
    "        'area': [\n",
    "            37, 14, 34,\n",
    "            16,\n",
    "        ],\n",
    "        'format': 'netcdf',\n",
    "    },\n",
    "    'Wind_press_Temp.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e25511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "\n",
    "c = cdsapi.Client()\n",
    "\n",
    "c.retrieve(\n",
    "    'reanalysis-era5-pressure-levels',\n",
    "    {\n",
    "        'product_type': 'reanalysis',\n",
    "        'format': 'netcdf',\n",
    "        'variable': 'relative_humidity',\n",
    "        'pressure_level': '100',\n",
    "        'year': '2021',\n",
    "        'month': [\n",
    "            '01', '02', '03',\n",
    "            '04', '05', '06',\n",
    "            '07', '08', '09',\n",
    "            '10', '11', '12',\n",
    "        ],\n",
    "        'day': [\n",
    "            '01', '02', '03',\n",
    "            '04', '05', '06',\n",
    "            '07', '08', '09',\n",
    "            '10', '11', '12',\n",
    "            '13', '14', '15',\n",
    "            '16', '17', '18',\n",
    "            '19', '20', '21',\n",
    "            '22', '23', '24',\n",
    "            '25', '26', '27',\n",
    "            '28', '29', '30',\n",
    "            '31',\n",
    "        ],\n",
    "        'time': [\n",
    "            '00:00', '01:00', '02:00',\n",
    "            '03:00', '04:00', '05:00',\n",
    "            '06:00', '07:00', '08:00',\n",
    "            '09:00', '10:00', '11:00',\n",
    "            '12:00', '13:00', '14:00',\n",
    "            '15:00', '16:00', '17:00',\n",
    "            '18:00', '19:00', '20:00',\n",
    "            '21:00', '22:00', '23:00',\n",
    "        ],\n",
    "    },\n",
    "    'RH.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ea779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netCDF4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1715f26",
   "metadata": {},
   "source": [
    "## Extraction of Required Variables From the NetCDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First importing the necessary modules to read and process the netcdf data\n",
    "#! pip install netCDF4\n",
    "import os, sys # will be used to create and modify file name and save it\n",
    "from netCDF4 import Dataset # Dataset method in netCDF4 library will be used to read .nc, .nc4 files\n",
    "import pandas as pd # pandas module will be used to read and modify dataframe\n",
    "from datetime import datetime,timedelta # will be used to manipulate date and time\n",
    "import matplotlib.pyplot as plt # will be used to plot the data\n",
    "import numpy as np\n",
    "import datetime # will be used to manipulate the datetime\n",
    "import warnings # Will be used to ignore the unnecessary warning so that the code look good esthetically\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a404cffe",
   "metadata": {},
   "source": [
    "## Understanding the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7a4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now reading the netCDF file from the directory where it is located, \"r\" stands for reading mode\n",
    "# Reading only one arbitrary file to observe the variables\n",
    "data = Dataset(\"F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\ERA5_RH_Malta1.nc\", 'r')\n",
    "# After reading the data as an object stored as 'data' here, printing the variables it has as follows:\n",
    "print(\"Here are all the variables of your data\")\n",
    "print(data.variables.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed48b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145463ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = data.variables['r'] \n",
    "rh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40746ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the unit of the precipitaiton mentioned in the file\n",
    "unitrh = data.variables['r'].units \n",
    "unitrh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the lat values and showing. Lats are ranging from -90 to +90\n",
    "lats = data.variables['latitude']\n",
    "lats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a944323",
   "metadata": {},
   "outputs": [],
   "source": [
    "lats = data.variables['latitude'][:]\n",
    "lats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the lon values and showing. Lons are ranging from -180 to +180\n",
    "lons = data.variables['longitude'][:] # total precipitation\n",
    "lons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed83881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the time steps and showing. Lons are ranging from -180 to +180\n",
    "times = data.variables['time'] # total precipitation\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00986cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading all the time steps and showing. Lons are ranging from -180 to +180\n",
    "times = data.variables['time'][:10] # total precipitation\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b09c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "unitt = data.variables['time'].units \n",
    "unitt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a56d5a4",
   "metadata": {},
   "source": [
    "#### Notice that the unit of time is in hours from the reference date time!\n",
    "From the time values and units. it's confirm that the data are stored at hourly intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d248f",
   "metadata": {},
   "source": [
    "## Clipping The NetCDF Files by Shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importig required modules\n",
    "#! pip install Fiona\n",
    "! pip install regionmask\n",
    "! pip install cartopy\n",
    "import xarray as xr \n",
    "import numpy as np\n",
    "import regionmask\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.filterwarnings(action='ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76282d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca91ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the shapefile and extracting the attributes \n",
    "shapefile = \"F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\MLT_adm\\\\MLT_adm0.shp\"\n",
    "countries = gpd.read_file(shapefile,crs=\"epsg:4326\")\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cadb4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting the study area\n",
    "fig, ax = plt.subplots(figsize=(16,10))\n",
    "countries.plot(ax=ax,column = 'ISO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1970a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the lat lons\n",
    "print(countries.loc[0,'geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16acfeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing the indeces \n",
    "my_list = list(countries['NAME_ISO'])\n",
    "my_list_unique = set(list(countries['NAME_ISO']))\n",
    "indexes = [my_list.index(x) for x in my_list_unique]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9101ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the bounday mask as polygon\n",
    "countries_mask_poly = regionmask.Regions(name = 'NAME_ISO', numbers = indexes, names = countries.NAME_ISO[indexes], abbrevs = countries.NAME_ISO[indexes], outlines = list(countries.geometry.values[i] for i in range(0,countries.shape[0])))\n",
    "countries_mask_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152ecf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the Name of the study area\n",
    "print(\"{}\".format(countries_mask_poly.names[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now reading the netcdf files using xarray\n",
    "# The following command is for data with both single and multiple varialbes\n",
    "ds = xr.open_dataset(\"F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\ERA5_RH_Malta1.nc\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b2ce03",
   "metadata": {},
   "source": [
    "### We can see that the NetCDF file contains only one variable which is Relative Humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083bb674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot relative Humidity for a time period\n",
    "plt.figure(figsize=(16,10))\n",
    "ax = plt.axes()\n",
    "ds.r.isel(time = 2500).plot(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bdfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the resolution of the data is coarse (i.e., 0.25 deg X 0.25 deg) and the size of the study area is very small,\n",
    "# We need to increase the resolution of the data in the NetCDF file.\n",
    "# Create new lat and lon\n",
    "dx_new = 0.05\n",
    "newlon = np.arange(14, 16, dx_new)\n",
    "newlat = np.arange(34, 37+dx_new, dx_new)\n",
    "\n",
    "# Interpolating the data onto the new grids\n",
    "data_set_interp = ds.interp(latitude=newlat, longitude=newlon)\n",
    "\n",
    "# Check output\n",
    "print(data_set_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ef7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the interpolated data again to see the changes in the resolution\n",
    "plt.figure(figsize=(16,10))\n",
    "ax = plt.axes()\n",
    "data_set_interp.r.isel(time = 2500).plot(ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d58d62",
   "metadata": {},
   "source": [
    "### We can notice that the resolution of the data has incresed significantly after the interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbafac52",
   "metadata": {},
   "source": [
    "### Test code for making interpolation\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Create sample data\n",
    "dx  = 0.25\n",
    "lon = np.arange(0, 360, dx)\n",
    "lat = np.arange(-90, 90+dx, dx)\n",
    "data = 10 * np.random.rand(len(lat), len(lon))\n",
    "data_set = xr.Dataset({\"temp\": ([\"lat\", \"lon\"], data)},\n",
    "                 coords={\"lon\": lon,\"lat\": lat})\n",
    "\n",
    "### Just checking the datasets are not empty\n",
    "print(data_set)\n",
    "\n",
    "### Create new lat and lon\n",
    "dx_new = 0.125\n",
    "newlon = np.arange(0, 360, dx_new)\n",
    "newlat = np.arange(-90, 90+dx_new, dx_new)\n",
    "\n",
    "### Interpolate\n",
    "data_set_interp = data_set.interp(lat=newlat, lon=newlon)\n",
    "\n",
    "### Check output\n",
    "print(data_set_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5567420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a mask based on the shapefile that will be used to clip the data from the .nc file\n",
    "print(\"Masking takes a while, please wait............\")\n",
    "mask = countries_mask_poly.mask(data_set_interp.isel(time = 0),lat_name='latitude', lon_name='longitude')\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62486c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The masked can be saved for future use as it always takes long time to generate. It's better to save\n",
    "mask.to_netcdf('F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\mask_by_Malta.nc') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3bfc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the mask from the disk as I saved it previously\n",
    "mask = xr.open_dataarray('F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\mask_by_Malta.nc')\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81505b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking the data based on the masked created using shapefile\n",
    "masked_shape = data_set_interp.where(mask == 0)\n",
    "masked_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa63e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the masked data for a time step only to check if the masking has been successfull\n",
    "plt.figure(figsize=(12,8))\n",
    "ax = plt.axes()\n",
    "masked_shape.r.isel(time = 5000).plot(ax = ax)\n",
    "countries.plot(ax = ax, alpha = 0.8, facecolor = 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50fe6c",
   "metadata": {},
   "source": [
    "### We notice that the masking is successfull using the shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc4991",
   "metadata": {},
   "source": [
    "Extracting time-series for one specific location\n",
    "In this example, we want to extract time-series for a location with the following latitude and longitude:\n",
    "longitude = 14.4; latitude = 35.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5872832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting time-series of relative humidity for the above-mentioned location\n",
    "latitude = 35.9\n",
    "longitude = 14.4\n",
    "data  = masked_shape.r.sel(longitude=longitude, latitude=latitude, method='nearest') \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ba50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from xarray and plotting\n",
    "df = data.to_dataframe()\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "df['r'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f34454f",
   "metadata": {},
   "source": [
    "### The data is for 5 years starting from 01 Jan 2017 to 31 Dec 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e867ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s plot montlhy distribution of relative humidity for Malta:\n",
    "df['Month'] = df.index.strftime(\"%b\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32aa811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the box-plot of monthly distribution\n",
    "#! pip install seaborn\n",
    "import seaborn as sns\n",
    "ax = plt.axes()\n",
    "sns.boxplot(x=\"Month\", y=\"r\", data=df, palette=\"Set1\")  \n",
    "figure = ax.get_figure()    \n",
    "figure.set_size_inches(12, 8) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf4b3a",
   "metadata": {},
   "source": [
    "## Everything Looks alright, Now saving the masked data as .nc format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334dfc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data to the same location as input\n",
    "encoding = {\"r\": {'zlib': True,\"complevel\": 4}}\n",
    "#format='NETCDF4', engine='netcdf4',encoding={'Tair': {'zlib': True,'dtype': 'float32', '_FillValue': -9999}}\n",
    "masked_shape.to_netcdf('F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\ERA5_RH_Malta_Clipped.nc',mode='w',format='NETCDF4', engine='netcdf4',encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0793d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can read the saved nc file and extract a time series as well\n",
    "# The following command is for data with single varialbe\n",
    "data = xr.open_dataarray('F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\ERA5_RH_Malta_Clipped.nc')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the masked data after reading from the clipped .nc file\n",
    "plt.figure(figsize=(16,10))\n",
    "ax = plt.axes()\n",
    "# Just plotting one variable\n",
    "data.isel(time = 2500).plot(ax = ax)\n",
    "countries.plot(ax = ax, alpha = 0.8, facecolor = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are extracting the time series for a single point and plotting the time series\n",
    "# the latitude and longitude of the point of interest are 35.90 and 14.4 degrees, respectively.\n",
    "single_point = data.sel(latitude=35.90, longitude=14.4, method ='nearest')\n",
    "single_point.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now clipping the climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now reading the netcdf files using xarray\n",
    "# The following command is for data with both single and multiple varialbes\n",
    "ds = xr.open_dataset(\"F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\ERA5_Climate_Malta.nc\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the resolution of the data is coarse (i.e., 0.25 deg X 0.25 deg) and the size of the study area is very small,\n",
    "# We need to increase the resolution of the data in the NetCDF file.\n",
    "# Create new lat and lon\n",
    "dx_new = 0.05\n",
    "newlon = np.arange(14, 16, dx_new)\n",
    "newlat = np.arange(34, 37+dx_new, dx_new)\n",
    "\n",
    "# Interpolating the data onto the new grids\n",
    "data_set_interp = ds.interp(latitude=newlat, longitude=newlon)\n",
    "\n",
    "# Check output\n",
    "print(data_set_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ba7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking the data based on the masked created using shapefile\n",
    "masked_shape = data_set_interp.where(mask == 0)\n",
    "masked_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8bd431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data to the same location as input\n",
    "encoding = {\"t2m\": {'zlib': True,\"complevel\": 4}}\n",
    "#format='NETCDF4', engine='netcdf4',encoding={'Tair': {'zlib': True,'dtype': 'float32', '_FillValue': -9999}}\n",
    "masked_shape.to_netcdf('F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\ERA5_Climate_Malta_Clipped.nc',mode='w',format='NETCDF4', engine='netcdf4',encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3869c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can read the saved nc file and extract a time series as well\n",
    "# The following command is for data with single varialbe\n",
    "data = xr.open_dataset('F:\\\\Bhushan_SP5_ERA5_Processing_Script\\\\ERA5_Climate_Malta_Clipped.nc')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting time-series of relative humidity for the above-mentioned location\n",
    "latitude = 35.9\n",
    "longitude = 14.4\n",
    "data  = masked_shape.t2m.sel(longitude=longitude, latitude=latitude, method='nearest') \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48432ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe from xarray and plotting\n",
    "df = data.to_dataframe()\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "df['t2m'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99485de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let’s plot montlhy distribution of relative humidity for Malta:\n",
    "df['Month'] = df.index.strftime(\"%b\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14840074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the box-plot of monthly distribution\n",
    "import seaborn as sns\n",
    "ax = plt.axes()\n",
    "sns.boxplot(x=\"Month\", y=\"t2m\", data=df, palette=\"Set1\")  \n",
    "figure = ax.get_figure()    \n",
    "figure.set_size_inches(12, 8) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7e386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
